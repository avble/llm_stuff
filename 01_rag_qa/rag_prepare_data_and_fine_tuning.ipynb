{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install simpletransformers pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "splits = {'train': 'plain_text/train-00000-of-00001.parquet', 'validation': 'plain_text/validation-00000-of-00001.parquet'}\n",
    "df = pd.read_parquet(\"hf://datasets/rajpurkar/squad/\" + splits[\"train\"])\n",
    "df_test = pd.read_parquet(\"hf://datasets/rajpurkar/squad/\" + splits[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data\n",
    "* for fine-tuning the pre-trained model\n",
    "* for creating the vector database\n",
    "\n",
    "# Note\n",
    "* Due to the limit of local GPU/CPU, only first 100 records is prepared for fine-tuning the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data and test for finetuning the pre-trained model\n",
    "import json\n",
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "\n",
    "for row in df[:20].iterrows():\n",
    "    ans = row[1][\"answers\"]\n",
    "    ans_txt = ''.join(map(str, ans[\"text\"]))\n",
    "    ans_start = int(ans[\"answer_start\"][0])\n",
    "    train_data.append({\"context\": row[1][\"context\"], \"qas\":[{\"id\": row[1][\"id\"], \"is_impossible\": False, \"question\": row[1][\"question\"], \"answers\": [{\"text\": ans_txt, \"answer_start\": ans_start}]}]})\n",
    "\n",
    "\n",
    "for row in df_test[:20].iterrows():\n",
    "    ans = row[1][\"answers\"]\n",
    "    ans_txt = ''.join(map(str, ans[\"text\"]))\n",
    "    ans_start = ans[\"answer_start\"].tolist()\n",
    "    test_data.append({\"context\": row[1][\"context\"], \"qas\":[{\"id\": row[1][\"id\"], \"is_impossible\": False, \"question\": row[1][\"question\"], \"answers\": [{\"text\": ans_txt, \"answer_start\": ans_start}]}]})\n",
    "\n",
    "\n",
    "with open('train.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open('test.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vector database\n",
    "+ Generate embedding vector and store it as vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vector database\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import json\n",
    "\n",
    "model_path = \"TencentBAC/Conan-embedding-v1\"\n",
    "\n",
    "model = SentenceTransformer(model_path, device=\"cpu\")\n",
    "\n",
    "with open(\"train.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "    embedding_data = []\n",
    "    for data in train:\n",
    "        context = data[\"context\"]\n",
    "        question = data[\"qas\"][0][\"question\"]\n",
    "        emb = model.encode(question).tolist()\n",
    "        embedding_data.append({\"context\": context, \"embedding\": emb})\n",
    "\n",
    "    with open(\"embedding.json\", \"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(embedding_data, f_out, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tunning the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simpletransformers.question_answering import QuestionAnsweringModel, QuestionAnsweringArgs\n",
    "import json, os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"False\"\n",
    "\n",
    "with open(\"train.json\", 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "with open(\"test.json\", 'r', encoding='utf-8') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "train_args = {\n",
    "    'overwrite_output_dir': True,\n",
    "    \"evaluate_during_training\": True,\n",
    "    \"max_seq_length\": 256,\n",
    "    \"num_train_epochs\": 2, #25, after experimentations\n",
    "    \"evaluate_during_training_steps\": 500,\n",
    "    \"save_model_every_epoch\": False,\n",
    "    \"save_eval_checkpoints\": False,\n",
    "    \"n_best_size\":16, #batch_size is another important argument\n",
    "    \"train_batch_size\": 16,\n",
    "    \"eval_batch_size\": 16,\n",
    "    \"output_dir\":\"./output\"\n",
    "}\n",
    "\n",
    "model = QuestionAnsweringModel(\"bert\",\n",
    "                               \"bert-large-cased\", \n",
    "                               args = train_args,\n",
    "                               use_cuda=True) # I will use GPU for faster performance\n",
    "\n",
    "model.train_model(train, eval_data=test, output_dir=\"./output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
